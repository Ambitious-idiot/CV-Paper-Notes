# Multiresolution KD for AD 笔记

## 一、背景

无监督特征学习在异常检测中起到重要作用，但是在相关数据的特性上存在很多挑战，一方面是样本绝对数量少导致模型难以泛化，另一方面是由于绝大多数样本均为正常样本导致正常异常样本比例极不均衡。现有模型多基于重建生成或特征提取，存在泛化问题、计算量过大、训练过程不稳定需要一些 trick的问题。

有人尝试在异常检测中使用预训练大模型，但他们的方法存在无法定位、基于区域处理等问题，导致对图片尺寸、分割尺寸等十分敏感而无法完全利用预训练模型的表示能力。

随着网络中间层表示能力的发掘，作者提出通过知识蒸馏将预训练模型的中间层表示嵌入小型网络实现对预训练模型表示能力的迁移。

## 二、主要思路

在已有的预训练教师模型内部选取若干目标层，通过知识蒸馏的思路使学生网络的若干中间层输出模拟教师模型的目标层，实现模型表示能力尤其是中间层表示能力的迁移。这个训练过程整体均在正常样本数据集上进行，使得教师网络与学生网络在正常样本上输出一致而在异常样本上输出不同，模型实际运用时只需要根据损失函数的计算即可进行异常检测和定位。

## 三、损失函数设计

以下符号意义如下：S下标表示教师网络，C下标表示学生网络，$CP_i$ 表示教师网络中选取的第 i 个目标层，$a_{net}^{layer}(j)$表示神经网络 net 的 layer 层第j个神经元的输出, $N_{CP}$表示目标层个数，$N_{i}$表示某层神经元个数。

损失函数由两部分组成，一方面为值距离量度$L_{val}$，一方面为方向距离量度$L_{dir}$:

1. $L_{val}=\sum^{N_{CP}}_{i=1}\frac{1}{N_i}\sum^{N_i}_{j=1}||a_S^{CP_i}(j)-a_C^{CP_i}(j)||^2$，表示两网络输出值的差异性；
2. $L_{dir}=\sum^{N_{CP}}_{i=1}{(1-cos\small<a_S^{CP_i}(j),a_C^{CP_i}(j)\small>)} = \sum^{N_{CP}}_{i=1}{(1-\frac{{a_S^{CP_i}}^T\cdot a_S^{CP_i}}{||a_S^{CP_i}||||a_S^{CP_i}||})}$，表示两网络输出的方向差异性。
3. 总损失$L_{tot}=L_{val}+\lambda L_{dir}$。

## 四、异常检测

根据三的损失函数，使用知识蒸馏的范式训练得到模型。进行异常检测时，只需要同时使用教师网络和学生网络计算输入的特征，并计算三中定义的损失。当损失函数大小超过某一阈值，则输入为异常样本。

## 五、异常定位

显然，在根据中间层的特征推得的损失中，其关于图片各点的偏导表示了其异常程度，因此可以根据偏导$\frac{\partial L_{tot}}{\partial x}$可以得到异常分数定位图。此后为了减少自然噪音的影响，对其作进一步的滤波计算得到最终定位图，具体方法见原文。